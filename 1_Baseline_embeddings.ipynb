{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TanmayKhot/AI_project/blob/main/1_Baseline_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reverse Image Search\n",
        "\n",
        "Reverse image search essentially also known as instance retrieval, enables us to build scenarios beyond just keywork search. Thus, here given an image as a query we output some other images that are similar and correlated to the query image. \n",
        "\n",
        "Since, we need to search among millions of images to find other similar images we ideally need to summarize the information contained in the millions of pixels in a image into a smaller representation. Deep neural networks now come into play. Using a Convolutional Neural Network an input image is converted into a feature vector of thousand dimensions, which is then supplied as an input query and returns the top matches for the query image.\n",
        "\n",
        "These feature vectors are also called as embeddings which is a collection of floating point values. Embeddings represent the most important and salient constituents of the image. "
      ],
      "metadata": {
        "id": "CCcopxdHKgrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to find similar images we pass the images through a pretrained convolutional neural network, ResNet-50 to extract feature embeddings. This is not optimal as res-net50 is not specilized in recognizing faces."
      ],
      "metadata": {
        "id": "27QFQhg_Sv6k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlg5gx3t8Ym6"
      },
      "outputs": [],
      "source": [
        "# Importing required libraries\n",
        "import json\n",
        "import tqdm\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "from multiprocessing import cpu_count\n",
        "from tqdm.contrib.concurrent import process_map\n",
        "import urllib.request\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from numpy.linalg import norm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset\n",
        "\n",
        "Labeled Faces in Wild (LFW) is the dataset we use for the project. LFW contains face photographs where each face has been labeled with the name of the person pictured. The data set contains more than 13,000 images of faces collected from the web.\n",
        "\n",
        "There are 4 different sets of LFW images. For our project, we work with the 'Deep Funneled' images. Deep funneled LFW dataset is formed by a combination of unsupervised joint alignment with unsupervised feature learning. \n",
        "\n",
        "We incorporate the deep funneled LFW dataset as it produces superior results over the original dataset."
      ],
      "metadata": {
        "id": "wGHQERvNVfFV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr_hNt818Ym-"
      },
      "source": [
        "## Extract data\n",
        "\n",
        "After having manually uploaded the .tgz file containing the data we need to unzip it "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "RvhX1EkW8YnD",
        "outputId": "591d31a1-291c-40d8-9f50-1cbab422402c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\r\n"
          ]
        }
      ],
      "source": [
        "!tar -xf lfw-deepfunneled.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_Pqoqxn8YnH"
      },
      "outputs": [],
      "source": [
        "#function to get path from each folder recursively \n",
        "extensions = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG']\n",
        "def get_file_list(root_dir):\n",
        "    file_list = []\n",
        "    counter = 1\n",
        "    for root, directories, filenames in os.walk(root_dir):\n",
        "        for filename in filenames:\n",
        "            if any(ext in filename for ext in extensions):\n",
        "                file_list.append(os.path.join(root, filename))\n",
        "                counter += 1\n",
        "    return file_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlMvuZIr8YnJ"
      },
      "outputs": [],
      "source": [
        "#provide path to main folder of dataset and get img paths to the datasets\n",
        "root_dir = os.getcwd()+ '/lfw-deepfunneled/'\n",
        "filenames = sorted(get_file_list(root_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "_GGIAuaM8YnL",
        "outputId": "d159b563-1c8c-4993-b2c4-4a373bbd4412"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/home/sb8389/A Projects/Reverse Image Project/lfw-deepfunneled/AJ_Cook/AJ_Cook_0001.jpg',\n",
              " '/home/sb8389/A Projects/Reverse Image Project/lfw-deepfunneled/AJ_Lamas/AJ_Lamas_0001.jpg',\n",
              " '/home/sb8389/A Projects/Reverse Image Project/lfw-deepfunneled/Aaron_Eckhart/Aaron_Eckhart_0001.jpg']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#see it list of paths\n",
        "filenames[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3evJoLf8YnM"
      },
      "source": [
        "## Prepare Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A deep learning model trained on a large set of labelled images have basically become an automatic “feature extractor”. This means that when analyzing our images, we basically get as output a “feature vector” which contains relevant information about the image content. Training a model from scratch can take up a lot of computing resources and time. Leveraging the concept of transfer learning helps us get a headstart into the training process. Transfer learning involves using models trained on one problem as a starting point on a related problem. Transfer learning has the benefit of decreasing the training time for a neural network model and can result in lower generalization error."
      ],
      "metadata": {
        "id": "yLJ3QXcXoozL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Residual Network (ResNet50):**\n",
        "\n",
        "For our project we use pre-trained weights from ResNet 50 as the baseline model to generate embeddings. ResNet is a CNN that is 50 layers deep. The pretrained version of the network is trained on more than a million images from the ImageNet database. The architecture is as follows:"
      ],
      "metadata": {
        "id": "pvB6k59dvao_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = 'https://drive.google.com/uc?id=1Gm6UQVJ7mvqyuW6aHoD-kh8PW21EGCCz'>"
      ],
      "metadata": {
        "id": "1_UPnxuH7vTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet ideally applies identity mapping, meaning that the input to some layer is passed directly or as a shortcut to some other layer. It works on the principle of skip connection. Skip connection is basically the identity mapping where the input from previous layer is added directly to the output of the other layer. "
      ],
      "metadata": {
        "id": "ff_NtXPyXhsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load ResNet-50 without the top classification layers, so as to get the feature embeddings only. The 'extract_features' function below performs the following operations:\n",
        "1. Load image from the image path\n",
        "2. Resize to the format supported by ResNet-50\n",
        "3. Pre-process the image\n",
        "4. Extract the features\n",
        "5. Normalize the features"
      ],
      "metadata": {
        "id": "5uXsoX_Z8ZAc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpMkA-JF8YnP",
        "outputId": "5786ee04-dd66-4318-dfa8-302daff624be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "channels_last\n"
          ]
        }
      ],
      "source": [
        "# Set the channel first for better performance\n",
        "from tensorflow.keras import backend\n",
        "backend.set_image_data_format('channels_last')\n",
        "print(backend.image_data_format())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOmjYfc58YnR"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False,input_shape=(224, 224,3),pooling='avg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-W_7Oky8YnT",
        "outputId": "a12e5826-93b5-493f-ba8c-a7421b505ad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory  model  Created \n"
          ]
        }
      ],
      "source": [
        "#Creating the directory structure\n",
        "dirName = 'model'\n",
        "if not os.path.exists(dirName):\n",
        "    os.makedirs(dirName)\n",
        "    print(\"Directory \" , dirName ,  \" Created \")\n",
        "else:    \n",
        "    print(\"Directory \" , dirName ,  \" already exists\")   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Q27csdB8YnU",
        "outputId": "2773f992-f68d-4f0e-9a53-a059aa859327"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./model/assets\n"
          ]
        }
      ],
      "source": [
        "#Save the model in SavedModel format\n",
        "model.save('./model/', save_format='tf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k17myLau8YnX"
      },
      "outputs": [],
      "source": [
        "# takes an image path, loads the image, resizes it to proper \n",
        "#dimensions supported by ResNet-50, extracts the features, and then normalizes them\n",
        "def extract_features(img_path, model):\n",
        "    input_shape = (224, 224,3)\n",
        "    img = image.load_img(img_path, target_size=(\n",
        "        input_shape[0], input_shape[1]))\n",
        "    img_array = image.img_to_array(img)\n",
        "    expanded_img_array = np.expand_dims(img_array, axis=0)\n",
        "    preprocessed_img = tf.keras.applications.resnet50.preprocess_input(expanded_img_array)\n",
        "    features = model.predict(preprocessed_img)\n",
        "    flattened_features = features.flatten()\n",
        "    normalized_features = flattened_features / norm(flattened_features)\n",
        "    return normalized_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILtai-9e8YnZ",
        "outputId": "5111e1b4-6c59-4f1b-f113-7385a3e4d295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2048\n"
          ]
        }
      ],
      "source": [
        "#let's try with one image\n",
        "features = extract_features('/home/sb8389/A Projects/Reverse Image Project/lfw-deepfunneled/AJ_Cook/AJ_Cook_0001.jpg', model)\n",
        "print(len(features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "61bc22e19c2c467b889c60a3ec2bbd25"
          ]
        },
        "id": "NCzNr9J08Ynb",
        "outputId": "b228ac04-50c1-432f-fbc3-042f86220688"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/stow/python-3.6/lib/python3.6/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61bc22e19c2c467b889c60a3ec2bbd25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=13233.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#get features for each pic from model\n",
        "feature_list = []\n",
        "for i in tqdm_notebook(range(len(filenames))):\n",
        "    feature_list.append(extract_features(filenames[i], model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukcUyK3A8Ynd"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "pickle.dump(feature_list, open('saved/features-lfw-deepfunneled-resnet50.pickle', 'wb'))\n",
        "pickle.dump(filenames, open('saved/filenames-lfw-deepfunneled.pickle','wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, we now have the embeddings from our ResNet-50 baseline model which we will upload to EC2."
      ],
      "metadata": {
        "id": "q1DIzPo2-U6e"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "1 - Baseline embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}