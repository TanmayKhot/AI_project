{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TanmayKhot/AI_project/blob/main/2_Facenet_Embeddings_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **DeepFace**\n",
        "\n",
        "DeepFace is an open source face recognition and facial attribute analysis library. It includes a lot of AI models for face recognition and automatically handles all procedures for facial recognition in the background. \n",
        "\n",
        "A modern face recognition pipeline consists of 5 common stages: detect, align, normalize, represent and verify. Deepface handles all these common stages in the background.\n",
        "\n",
        "Using DeepFace gives us access to the following set of features:\n",
        "\n",
        "1. Face Verification: The task of face verification refers to comparing a face with another to verify if it is a match or not. \n",
        "2. Face Recognition: This refers to finding a face in the database.\n",
        "3. Facial Attribute Analysis: This task refers to describing the visual properties of face images. For example emotion analysis, etc\n",
        "4. Real-Time Face Analysis: This includes testing the above tasks with real time video feed.\n",
        "\n",
        "There are various deep learning algorithms that can be used with the DeepFace library. These are based on Convolutional Neural Networks (CNN). Some of the models are as follows:\n",
        "\n",
        "1. VGG Face: The DeepFace library uses VGG-Face as its default model. It is based on deep convolutional neural networks. It has the same structure as the regular VGG model except it is fine-tuned for images.\n",
        "\n",
        "2. OpenFace: OpenFace is an open source tool heavily inspired by FaceNet. \n",
        "\n",
        "3. DeepID: It is a face verification algorithm based on deep learning. It is an external face recognition model wrapped DeepFace Library.\n",
        "\n",
        "4. Dlib: This face recognition model is used to recognize and manipulate faces. Dlib’s face recognition tool maps an image of a human face to a 128-dimensional vector space and uses Euclidean distance to compute similarity.\n",
        "\n",
        "5. Arcface: Traditionally for face recognition tasks, softmax is used. Sometimes it leads to a performance gap for deep face recognition under large intra-class appearance variations. In this case ArcFace, or Additive Angular Margin Loss is used to solve this discrepancy.\n",
        "\n",
        "\n",
        "There are many such models. However for our project we use the *FaceNet model* to improve the performance over our baseline model and get better results."
      ],
      "metadata": {
        "id": "zZ6Zl9hNFuC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **FaceNet**\n",
        "\n",
        "FaceNet was developed at Google. It can be used for face recognition, verification and clustering. The main reason we chose this model is its high efficiency and performance. It is reported to achieve 99% accuracy on the LFW dataset. It is 22 layers deep neural network that directly trains its output to be a 128-dimension embedding. The loss function used at the last layer is called triplet loss. \n",
        "\n",
        "The FaceNet model structure is as follows:\n",
        "\n",
        "<img src = 'https://drive.google.com/uc?id=1j9-ezNOykuwZxinCp17hb4TQN9geushY' height = 230>\n",
        "\n",
        "FaceNet is based on the idea of a Siamese Network.\n",
        "\n",
        "**Siamese Network:** Also called as twins neural network. It involvs the idea that any pair fed to the neural network leads to an output in the form of its features. The distance of the two outputs is calculated to compare their features. The distance indicates the similarity between them; larger it is, higher probability they belong to the same class.\n",
        "\n",
        "There is a direct and an indirect method to validate the model. \n",
        "\n",
        "The direct method comes into play for FaceNet. Here you find the correct class of the picture. This approach takes any picture, let’s call it A, calculates the similarity_score between A to some randomly picked pictures such that this group has only one same class to A and all other are in different classes. The predicted class is defined by the picture which has highest similarity_score with A.\n",
        "\n",
        "Well known distance measures like Euclidean distance is used to measure the distance between the embeddings extracted.\n",
        "\n",
        "Another important aspect of FaceNet is the loss function as follows:\n",
        "\n",
        "**Triplet Loss:** So the CNN is trained with triple images at each step as\n",
        "1. Anchor\n",
        "2. Positive\n",
        "3. Negative\n",
        "\n",
        "<img src = 'https://drive.google.com/uc?id=1g0TsBCTe7LEjbFAeAz7Ti3y-Yc046RZ2' height = 170>\n",
        "\n",
        "The anchor and positive image belong to the same class and negative image belongs to a different class. Now, the intuition behind triplet loss is that we want our anchor image to be closer to positive images as compared to negative. \n",
        "\n",
        "The loss function is formally defined as:\n",
        "\n",
        "$$ \\sum_{i}^{N} [||f(x_i^a) - f(x_i^p)||_2^2 - ||f(x_i^a) - f(x_i^n)||_2^2 + \\alpha] $$\n",
        "\n",
        "where...\n",
        "\n",
        "$x_i$ -> represents an image\n",
        "\n",
        "$f(x_i)$ -> represents embedding of an image\n",
        "\n",
        "$\\alpha$ -> represents margin between positive and negative pairs"
      ],
      "metadata": {
        "id": "fUYCrjdWldQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's extract embeddings using FaceNet!"
      ],
      "metadata": {
        "id": "a1ot80ti0rmW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29nusjfr8mdX"
      },
      "outputs": [],
      "source": [
        "# Importing required libraries\n",
        "\n",
        "from deepface import DeepFace\n",
        "import os\n",
        "import pickle\n",
        "from tqdm.notebook import tqdm_notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zsIgKLA8mda"
      },
      "source": [
        "Let's load the location of our pics computer while creating the previous embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUbu1NaQ8mdb"
      },
      "outputs": [],
      "source": [
        "#function to load the previously saved files\n",
        "def load_pickle(file):\n",
        "    objects = []\n",
        "    with (open(file, \"rb\")) as openfile:\n",
        "        while True:\n",
        "            try:\n",
        "                objects.append(pickle.load(openfile))\n",
        "            except EOFError:\n",
        "                break\n",
        "    return objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v8OxPx28mdc"
      },
      "outputs": [],
      "source": [
        "filenames = load_pickle('saved/filenames-lfw-deepfunneled.pickle')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXyQZT4K8mdc",
        "outputId": "8020136f-04f1-4486-b270-58d30004e234"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "13233"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(filenames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLmMWCSQ8mdd"
      },
      "source": [
        "Now that we have a list with file locations let's create embeddings with facenet and deepface"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use .represent(): This function is used to represent facial images as vectors / embeddings. \n",
        "\n",
        "Parameters:\n",
        "\n",
        "img_path: exact image path, numpy array or based64 encoded images could be passed.\n",
        "\n",
        "model_name (string): VGG-Face, Facenet, OpenFace, DeepFace, DeepID, Dlib, ArcFace.\n",
        "\n",
        "model: Built deepface model. A face recognition model is built every call of verify function unless a model is passed. Given that we don't need to build the model at every call we prebuild it by calling  `model = DeepFace.build_model('Facenet')` and passing it in the function.\n",
        "      \n",
        "enforce_detection (boolean): If any face could not be detected in an image, then verify function will return exception. Set this to False not to have this exception. This might be convenient for low resolution images."
      ],
      "metadata": {
        "id": "5oGkLdhy5tiR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "CvuA91pM8mdd"
      },
      "outputs": [],
      "source": [
        "#let's build the model we want to use:\n",
        "model = DeepFace.build_model('Facenet')\n",
        "\n",
        "#function to get embeddings given an image\n",
        "def extract_features(image_path, model):\n",
        "    vector = DeepFace.represent(img_path = image_path, model_name = \"Facenet\", model = model, enforce_detection = False)\n",
        "\n",
        "    return vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dimensions of the Embedding:\n",
        "\n",
        "The embedding size for FaceNet is 128. Now, larger embeddings require more training to reach a certain level of accuracy. However, for FaceNet a 128 dimensional float vector can be quantized to 128 bytes without any loss of accuracy ! Thus large scale clustering and recognition becomes easier."
      ],
      "metadata": {
        "id": "fi2zXOYhw2OJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "a2b6863157fa4425a3ca52acfabfd08b"
          ]
        },
        "id": "1AzfNZvB8mde",
        "outputId": "bc10831a-b9e2-4cb2-cbe6-d5f969f83717"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2b6863157fa4425a3ca52acfabfd08b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=13233.0), HTML(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "feature_list = []\n",
        "for i in tqdm_notebook(range(len(filenames))):\n",
        "    feature_list.append(extract_features(filenames[i], model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mpNquFF8mde"
      },
      "outputs": [],
      "source": [
        "#let's save the embedding in a file, to import in ec2\n",
        "pickle.dump(feature_list, open('saved/features-facenet.pickle', 'wb'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGxqbzRR8mdf"
      },
      "outputs": [],
      "source": [
        "#let's normalize and pickle again\n",
        "from numpy.linalg import norm\n",
        "for i in range(len(feature_list)):\n",
        "    feature_list[i] = feature_list[i] / norm(feature_list[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C2MOU1k8mdf"
      },
      "outputs": [],
      "source": [
        "pickle.dump(feature_list, open('saved/features-facenet-normalized.pickle', 'wb'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nzBfmVI8mdf"
      },
      "source": [
        "Now that we have embeddings generated by FaceNet model we download the pickle file and upload it in EC2 and continue there.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "2 - Facenet Embeddings .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}